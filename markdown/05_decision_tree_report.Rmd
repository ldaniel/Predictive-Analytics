---
title: "The decision tree on loan's report"
date: "August, 2019"
---

```{r setup_logit, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(plotly)

# libraries for modeling
library(caret)
library(gmodels)
library(rpart)
library(rpart.plot)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```

# Objective

The goal of this session is trying to fit a simple Decision Tree model on Loan data aiming to predict the probability of delinquency for each contract.

*******************************************************************************

# Modeling

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.

```{r variables, echo=TRUE}
loan_dataset_logistic <- source_dataset

kable(tibble(variables = names(loan_dataset_logistic)))
```

## Variable selection

One advantage of Decision Tree models is that it does not require heavy feature engineering.

We will only remove **x_prop_old_age_pension** that we know beforehand to have no variance in the dataset.

This model is also not sensible to outliers, missing values and multicollinearity.

```{r variable_selecion, echo=TRUE}
loan_dataset_DT <-source_dataset
loan_dataset_DT <- dplyr::select(loan_dataset_DT, -x_prop_old_age_pension)
```

## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

We are selecting exact the same samples we used for the Logistic Model to allow comparison across models.

```{r sampling, echo=TRUE}
SplitDataset <- source_train_test_dataset
data.train_DT <- SplitDataset$data.train
data.test_DT <- SplitDataset$data.test

data.train_DT <- dplyr::select(data.train_DT, names(loan_dataset_DT))
data.test_DT <- dplyr::select(data.test_DT, names(loan_dataset_DT))

kable(SplitDataset$event.proportion)
```

Both datasets keep the same proportion for the explained variable around 11%.

## Fiting the Decision Tree model 

With the final cleaned dataset, we got from above steps we fit our Decision Tree Model for **y_loan_defaulter** on all **x_variables**.

We choose to change the default parameters **minbucket** and **maxdepth** parameter to **5** to deliberate create a fully-grown tree as the database is not big enough for this model.

```{r fit_DT_full, eval=FALSE}
tree.full <- rpart(data= data.train_DT, y_loan_defaulter ~ .,
                   control = rpart.control(minbucket = 5,
                                           maxdepth = 5),
                   method = "class")

rpart.plot(tree.full, cex = 1.3, type = 0,
           extra = 1, box.palette = 'BuRd',
           branch.lty = 3, shadow.col = 'gray', 
           nn = TRUE, main = 'Decision Tree - Full')

# save model
saveRDS(tree.full, './models/decision_tree_full.rds')
```

```{r load_DT_full, echo=TRUE, include=FALSE}
# load model
tree.full <- readRDS('./models/decision_tree_full.rds')
```

```{r plot_DT_full, echo=FALSE, out.width= '100%'}
rpart.plot(tree.full, cex = 1.3, type = 0,
           extra = 104, box.palette = 'BuRd',
           branch.lty = 3, shadow.col = 'gray', 
           nn = TRUE, main = 'Decision Tree - Full')
```

## Evaluating necessity of prunning

Pruning is a technique in machine learning used to reduce the size of decision trees by removing nodes that provide little power of classification.

The idea is reducing the complexity of the decision tree and thereof avoid overfit to the train dataset.

Pruning process is done by comparing different variations of the fully-grown tree and evaluating the relative error trend comparing to a zero node tree, each node that does not affect the classification power of the tree is replaced by a leaf node.

```{r fit_DT_prune, eval=FALSE}
printcp(tree.full)
plotcp(tree.full)

cp_prune = tree.full$cptable[which.min(tree.full$cptable[,"xerror"]), "CP"]
tree.prune <- prune(tree.full, cp = cp_prune)

rpart.plot(tree.prune, cex = 1.3, type = 0,
           extra = 104, box.palette = 'BuRd',
           branch.lty = 3, shadow.col = 'gray', 
           nn = TRUE, main = 'Decision Tree - Prune')

# save model
saveRDS(tree.prune, './models/decision_tree_prune.rds')
```

```{r load_DT_prune, include=FALSE}
# load model
tree.prune <- readRDS('./models/decision_tree_prune.rds')
```

```{r plot_DT_prune, echo=TRUE, out.width= '100%'}
rpart.plot(tree.prune, cex = 1.3, type = 0,
           extra = 104, box.palette = 'BuRd',
           branch.lty = 3, shadow.col = 'gray', 
           nn = TRUE, main = 'Decision Tree - Prune')
```

On performing the pruning process in our decision tree, we end up with a tree with only one node split.
This is because splitting the train data set of just one node provide an overall accuracy greater than any other combination.

Here the technique falls short for this dataset, we would be interest in give a greater weight on correctly predict the real defaulters than the general accuracy of the model.

A Decision Tree is definitely not a good model for this dataset, but as we are just exercising the modeling technique (just having fun here !!!) we will use the full decision tree we got to compare its metrics and see how it perform against the other models we created in this class exercise.

We tried different parameters for **minbucket** and **maxdepth**, as our dataset is not big enough all of then presented huge differences in performance, the one we chose for this exercise seems to be a reasonable trade-off on overfitting to compare against the other models.

We can do this by simply feeding back the full tree to our prune tree object and move on to the performance metrics.

```{r fit_DT_prune_1, echo=TRUE, out.width= '100%'}
tree.prune <- tree.full
```

*******************************************************************************

# Interpreting model output

Decision Trees are known by its very descriptive rules on how it is classifying each observation.
It provides a very clear human readable set of rules that can show the importance of each variable on the decision process.

Our full decision tree model clear shows that below variables are the key features that can be used to decide the likelihood of default in each contract:

- **x_account_balance**
- **x_prop_interest_credited**
- **x_avg_account_balance**
- **x_loan_amount**
- **x_no_of_commited_crimes_1995**

The result is similar to the Logistic Regression model we created in the session before.

The predictors selected are roughly the same.

*******************************************************************************

# Evaluating the model performance

Here we will perform basically the same steps we did in the Logistic Regression model.

A comparison against all the models will be provided in the Final Report session of this exercise.

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.

```{r get_scores, echo=TRUE, out.width= '100%'}
## making preditions for each model and consolidating in a single data frame

prob.full = list()
prob.train = list()
prob.test = list()

prob.full$decision.tree.actual     <- loan_dataset_DT$y_loan_defaulter
prob.full$decision.tree.predicted  <- predict(tree.prune, type = "prob", newdata = loan_dataset_DT)[, 2]

prob.train$decision.tree.actual    <- data.train_DT$y_loan_defaulter
prob.train$decision.tree.predicted <- predict(tree.prune, type = "prob", newdata = data.train_DT)[, 2]

prob.test$decision.tree.actual     <- data.test_DT$y_loan_defaulter
prob.test$decision.tree.predicted  <- predict(tree.prune, type = "prob", newdata = data.test_DT)[, 2]

prob.full   <- prob.full %>% as_tibble()
prob.train  <- prob.train %>% as_tibble()
prob.test   <- prob.test %>% as_tibble()
```

## Getting performance measures

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------
metricsByCutoff.test_DT     <- modelMetrics(prob.test$decision.tree.actual, 
                                            prob.test$decision.tree.predicted, 
                                            plot_title = 'Decision Tree')
metricsByCutoff.test_DT$Plot
```

With the optimized cut off we calculate the full set of model metrics using the function HMeasure from hmeasure library.

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# decision tree
measures.decision.tree.train <- HMeasure(prob.train$decision.tree.actual, 
                                         prob.train$decision.tree.predicted, 
                                         threshold = metricsByCutoff.test_DT$BestCut['Cut'])
measures.decision.tree.test <- HMeasure(prob.test$decision.tree.actual, 
                                        prob.test$decision.tree.predicted, 
                                        threshold = metricsByCutoff.test_DT$BestCut['Cut'])

# join measures in a single data frame
measures <- t(bind_rows(measures.decision.tree.train$metrics,
                        measures.decision.tree.test$metrics
                        )) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('decision.tree - train', 'decision.tree - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())
```

Below are the metrics on the train and test dataset:
```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

## Evaluating classification performance

In general, such as the Logistic Regression model, this model is not delivering good accuracy, we will compare how it performed against other classes of models in the Final Report session

Below the confusion matrix and general performance of the model:

```{r see_accuracy_1, echo=TRUE, out.width= '100%'}
# accuracy metrics ---------------------------------------------------------------
# logistic regression
accuracy(score = prob.test$decision.tree.predicted, 
         actual = prob.test$decision.tree.actual, 
         threshold = metricsByCutoff.test_DT[["BestCut"]][["Cut"]])
```

We finally look at the score distribution charts to check how well the model is able to discriminate Defaulters and Non-Defaulters.

```{r model_plots, echo=TRUE, out.width= '100%'}
p1 <- Score_Histograms(prob.test, 
                 prob.test$decision.tree.predicted,
                 prob.test$decision.tree.actual,
                 'Density Plot') + theme(axis.title.y = element_blank())

p2 <- Score_Boxplot(prob.test, 
              prob.test$decision.tree.predicted, 
              prob.test$decision.tree.actual,
              'Score Boxplot')

p3 <- KS_Plot(prob.test$decision.tree.predicted[prob.test$decision.tree.actual == 0],
        prob.test$decision.tree.predicted[prob.test$decision.tree.actual == 1],
        'KS Plot') + theme(axis.title.y = element_blank(),
                           axis.text.y = element_blank())

ggarrange(p1, p2, nrow = 2)

p3
```

By the score density we see that our Decision Tree is very narrow on the scores it assigns to the observations.

This is expected from single Decision Trees.

The box plot also does not show a clear discrimination between Defaulters and Non-Defaulters.

Finally, the KS metric is far from what is expected for a reasonable classification model.

In the Final Report session, we will look more closely on the AUC and Gini metrics by plotting the ROC curve and comparing against other models.

Stay tuned!!!