---
title: "The logistic regression on loan's report"
date: "August, 2019"
---

```{r setup_logit, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```

# Objective

The goal of this report is trying to fit a logistic regression model on Loan data aiming to predict the probability of delinquency for each contract.

*******************************************************************************

# Task description

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.


```{r variables, echo=TRUE}
loan_dataset_logistic <- source_dataset

kable(tibble(variables = names(loan_dataset_logistic)))
```

## Variable selection

Starting from this dataset we investigate the presence of redundant or variables not useful for the model such as the ones with not enough variability.

Starting with the dummy variables.

We will keep only dummies that has the event in at least 5% in the dataset:

```{r dummies_high, echo=TRUE}
dummy_variables <- dplyr::select(loan_dataset_logistic, 
                                 starts_with('x_client_gender'), 
                                 starts_with('x_district_name'),
                                 starts_with('x_region'),
                                 starts_with('x_card_type'))

dummy_variables_high <- tibble(variables = names(dummy_variables),
       zeros = sapply(dummy_variables, 
                      function(x) table(as.character(x) == 0)["TRUE"]),
       ones = sapply(dummy_variables, 
                     function(x) table(as.character(x) == 1)["TRUE"])) %>% 
  mutate(prop_ones = round(ones / (zeros + ones) * 100, 2)) %>% 
  arrange(prop_ones) %>% 
  filter(prop_ones  > 5)

kable(dummy_variables_high)
```


The remaining dummies will be excluded from the dataset as they do not have enough variability to fit a logistic model on them.

```{r dummies_reject, echo=FALSE}
dummy_variables_high <- dummy_variables_high$variables
dummy_variables_low <- names(dplyr::select(dummy_variables, -dummy_variables_high))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -dummy_variables_low)
```

After we investigated the low variability in the dummies we look at the calculated variables on the transaction type proportion we calculated dutring the data enhancement process.

```{r prop, echo=TRUE}
prop_variables <- dplyr::select(loan_dataset_logistic, 
                                starts_with('x_prop'))

prop_variables <- summary(prop_variables)

kable(t(prop_variables))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_prop_old_age_pension)
```

Variable **x_prop_old_age_pension** is also excluded from the data set as in the dataset no observation has this type of transaction.

We ended up having a data set with **40 variables**.

```{r variables_2, echo=FALSE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Investigating Multicollinearity

With the remaining variables we ran a multicollinearity test to identify additional variables to drop from the model specification.

```{r check_correl, out.width = '100%', warning=FALSE}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)
VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_Before <- tibble(variable = names(VIF$idiags[,1]),
                    VIF = VIF$idiags[,1]) %>% 
             arrange(desc(VIF))

knitr::kable(VIF_Table_Before)
```

Then we look at the correlogram in the high VIF variables.

```{r check_correl_2, out.width = '100%'}
low_VIF <- filter(VIF_Table_Before, VIF <= 5)$variable
high_VIF <- filter(VIF_Table_Before, VIF > 5)$variable

high_VIF_dataset <- dplyr::select(loan_dataset_logistic, high_VIF)

cor_mtx_high_VIF <- cor(high_VIF_dataset)

high_VIF_correlogram_before <- ggcorrplot(cor_mtx_high_VIF, 
                                hc.order = TRUE,
                                lab = FALSE,
                                lab_size = 3, 
                                method="square",
                                colors = c("tomato2", "white", "springgreen3"),
                                title="Correlation Matrix of Loan Dataset Variables with high VIF") +
  theme(axis.text = element_blank(),
        legend.position = 0)

print(high_VIF_correlogram_before)

```

Then we exclude the variables with high correlation keeping the variables with less correlation with the others.

```{r check_correl_3, out.width = '100%'}
correl_threshold <- 0.6

reject_variables_vector <- tibble(var_1 = row.names(cor_mtx_high_VIF)) %>% 
  bind_cols(as_tibble(cor_mtx_high_VIF)) %>% 
  melt(id = c("var_1")) %>% 
  filter(var_1 != variable) %>%
  mutate(abs_value = abs(value)) %>%
  filter(abs_value > correl_threshold) %>%
  group_by(var_1) %>% 
  mutate(sum_1 = sum(abs_value)) %>% 
  ungroup() %>% 
  group_by(variable) %>% 
  mutate(sum_2 = sum(abs_value)) %>% 
  ungroup() %>% 
  mutate(reject = ifelse(sum_1 > sum_2, var_1, as.character(variable))) %>% 
  distinct(reject)

reject_variables_vector <- reject_variables_vector$reject

clean_dataset <- dplyr::select(loan_dataset_logistic, -reject_variables_vector)

kable(reject_variables_vector)
```

We finally look on the complete dataset correlogram with the clean data set.

```{r check_correl_4, out.width = '100%'}
cor_mtx_full <- cor(loan_dataset_logistic)
cor_mtx_clean <- cor(clean_dataset,)

full = ggcorrplot(cor_mtx_full, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Full Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

clean = ggcorrplot(cor_mtx_clean, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Clean Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

print(ggarrange(full, clean))

loan_dataset_logistic <- clean_dataset
```

We also look again at the VIF estimates for the clena dataset.

```{r check_correl_5, out.width = '100%'}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_average_salary)
```

We finally texclude one last variable that still has a VIF higher than our threshold (5)

```{r check_correl_6, echo=FALSE, out.width = '100%'}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")
```

After variable selection step we end up with **28 variables** in the dataset.

```{r variables_final, echo=TRUE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

```{r split_sample}
SplitDataset <- source_train_test_dataset
data.train_logistic <- SplitDataset$data.train
data.test_logistic <- SplitDataset$data.test

data.train_logistic <- dplyr::select(data.train_logistic, names(loan_dataset_logistic))
data.test_logistic <- dplyr::select(data.test_logistic, names(loan_dataset_logistic))

kable(SplitDataset$event.proportion)
```

Both datasets keep the same proportion for the explained variable around 11%.

## Fit the logistic model 

With the final cleaned dataset, we got from above steps we fit our Logistic Regression Y_loan_defaulter on all x variables.

```{r fit_model_full, echo=TRUE}
logistic.full <- glm(formula = y_loan_defaulter ~ .,
                     data= data.train_logistic, 
                     family= binomial(link='logit'))

names(logistic.full$coefficients) <- stringr::str_sub(names(logistic.full$coefficients), 1, 25)
summary(logistic.full)
```

Alternatively we fit a second model only with variables statistically significant p-value less than 10% using the stepwise method.

```{r fit_model_step_0, include=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
summary(logistic.step)
```

```{r fit_model_step_1, eval=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
```

```{r fit_model_step_show, echo=FALSE}
summary(logistic.step)
```

# Evaluating the model performance

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.

```{r get_scores, echo=TRUE}
## making preditions for each model and consilidating in a single data frame

prob.full = list()
prob.train = list()
prob.test = list()

prob.full$logistic.actual     <- loan_dataset_logistic$y_loan_defaulter
prob.full$logistic.predicted  <- predict(logistic.step, type = "response", 
                                         newdata = loan_dataset_logistic)

prob.train$logistic.actual    <- data.train_logistic$y_loan_defaulter
prob.train$logistic.predicted <- predict(logistic.step, type = "response", 
                                         newdata = data.train_logistic)

prob.test$logistic.actual     <- data.test_logistic$y_loan_defaulter
prob.test$logistic.predicted  <- predict(logistic.step, type = "response", 
                                         newdata = data.test_logistic)

prob.full   <- prob.full %>% as_tibble()
prob.train  <- prob.train %>% as_tibble()
prob.test   <- prob.test %>% as_tibble()
```

## Getting Performance Measures:

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

We will use a custom function developed in this paper to calculate the best cut off for each model:

```{r get_measures_functions, eval=FALSE}
## making preditions for each model and consilidating in a single data frame
# calculateModelMetrics -------------------------------------------------------
# The objective of this function is to calculate main metrics of model performance according to a cutoff value.
calculateModelMetrics <- function(cutData, realData, predData){
  cuttedData <- as.factor(ifelse(predData>=cutData, 1, 0))
  
  invisible(capture.output(out <- CrossTable(realData, cuttedData, 
                                             prop.c = F, prop.t = F, prop.r = T, prop.chisq = F)))
  
  out <- as.data.frame(out) %>% 
    mutate(merged=paste0(t.x, t.y)) %>% 
    dplyr::select(merged, val=t.Freq)
  
  TN <- filter(out, merged == "00")$val[1]
  FP <- filter(out, merged == "01")$val[1]
  FN <- filter(out, merged == "10")$val[1]
  TP <- filter(out, merged == "11")$val[1]
  
  return(data.frame(Cut = cutData,
                    TN = TN, 
                    FP = FP,
                    FN = FN, 
                    TP = TP,
                    TPR = TP/(TP+FN), TNR=TN/(TN+FP),
                    Error = (FP+FN)/(TP+TN+FP+FN),
                    Precision = TP/(TP+FP),
                    F1 = 2*(TP/(TP+FN))*(TP/(TP+FP))/((TP/(TP+FP)) + (TP/(TP+FN)))))
}

# modelMetrics ----------------------------------------------------------------
# The objective of this function is to calculate main metrics of model performance 
# for cutoffs from 0-1 based on given step.
modelMetrics <- function(realData, predData, stepping = 0.01, 
                         plot_title = "TPR/TNR by cutoff over full dataset"){
  probCuts <- seq(from = 0, to = 1, by = stepping)
  out <- bind_rows(lapply(probCuts, calculateModelMetrics, realData = realData, predData = predData))
  out <- out[complete.cases(out),] %>% mutate(Difference = abs(TPR-TNR))
  
  best <- out %>% arrange(Difference) %>% head(1) %>% dplyr::select(-Difference)
  
  p <- plot_ly(x = ~out$Cut, y = ~out$Difference, name = 'Abs. Diff.', type = 'bar', opacity = 0.3) %>% 
    add_trace(x = ~out$Cut, y = ~out$TPR, name = 'TPR', type = 'scatter', mode = 'lines', opacity = 1) %>% 
    add_trace(x = ~out$Cut, y = ~out$TNR, name = 'TNR', type = 'scatter', mode = 'lines', opacity = 1) %>% 
    add_text(x = best$Cut, y = best$TPR, text = best$Cut, opacity = 1) %>% 
    layout(xaxis = list(title = "Cutoff Value"),
           yaxis = list(title = "True Ratio (%)"),
           title = plot_title)
  
  return(list(TableResults = out,
              BestCut = best,
              Plot = p))
}

```

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------

metricsByCutoff.test_log    <- modelMetrics(prob.test$logistic.actual, 
                                            prob.test$logistic.predicted, 
                                            plot_title = 'Logistic Regression')
metricsByCutoff.test_log$Plot
```

With the optimized cut off we calculate the full set of model metrics.

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# logistic regression
measures.logistic.train <- HMeasure(prob.train$logistic.actual, 
                                    prob.train$logistic.predicted, 
                                    threshold = metricsByCutoff.test_log$BestCut['Cut'])
measures.logistic.test <- HMeasure(prob.test$logistic.actual, 
                                   prob.test$logistic.predicted, 
                                   threshold = metricsByCutoff.test_log$BestCut['Cut'])

# join measures in a single data frame
measures <- t(bind_rows(measures.logistic.train$metrics,
                        measures.logistic.test$metrics)
              ) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('logistic - train', 'logistic - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())

```

Below are the metrics on the train and test dataset:
```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

In general, this model has not delivered a good accuracy, we will compare how it performed against other classes of models in the Final Report section.

Below the confusion matrix and general performance of the model that we get from our custom function:
```{r see_accuracy, eval=FALSE}
accuracy <- function(score, actual, threshold = 0.5) {
  
  fitted.results <- ifelse(score > threshold ,1 ,0)
  
  misClasificError <- mean(fitted.results != actual)
  
  misClassCount <- misclassCounts(fitted.results, actual)
  
  print(kable(misClassCount$conf.matrix))
  
  print('--------------------------------------------------------------')
  print(paste('Model General Accuracy of: ', 
              round((1 - misClassCount$metrics['ER']) * 100, 2), '%', 
              sep = ''))
  print(paste('True Positive Rate of    : ', 
              round(misClassCount$metrics['TPR'] * 100, 2), '%',
              sep = ''))
}
```

```{r see_accuracy_1, echo=TRUE, out.width= '100%'}
# accuracy metrics ---------------------------------------------------------------
# logistic regression
accuracy(score = prob.test$logistic.predicted, 
         actual = prob.test$logistic.actual, 
         threshold = metricsByCutoff.test_log[["BestCut"]][["Cut"]])
```

We can finally look at the score distribution charts how well the model is able to split Defaulters and Non-Defaulters

```{r model_plots, echo=TRUE, out.width= '100%'}
p1 <- Score_Histograms(prob.test, 
                 prob.test$logistic.predicted,
                 prob.test$logistic.actual,
                 'Density Plot') + theme(axis.title.y = element_blank())

p2 <- Score_Boxplot(prob.test, 
              prob.test$logistic.predicted, 
              prob.test$logistic.actual,
              'Score Boxplot')

p3 <- KS_Plot(prob.test$logistic.predicted[prob.test$logistic.actual == 0],
        prob.test$logistic.predicted[prob.test$logistic.actual == 1],
        'KS Plot') + theme(axis.title.y = element_blank(),
                           axis.text.y = element_blank())

ggarrange(p1, p2, p3, nrow = 3)
```