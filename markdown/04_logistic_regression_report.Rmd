---
title: "The logistic regression on loan's report"
date: "August, 2019"
---

```{r setup_logit, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries
library(rmarkdown)
library(dplyr)
library(tidyr)
library(forcats)
library(lubridate)
library(tinytex)
library(feather)
library(corrplot)
library(mctest)
library(caret)
library(hmeasure)
library(pROC)
library(rms)
library(knitr)
library(ggplot2)
library(ggcorrplot)
library(ggthemes)
library(reshape2)
library(ggpubr)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
```

# Objective

The goal of this report is trying to fit a logistic regression model on Loan data aiming to predict the probability of delinquency for each contract.

*******************************************************************************

# Task description

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.


```{r variables, echo=TRUE}
loan_dataset_logistic <- DataPrep()
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Variable selection

Starting from this dataset we investigate the presence of redundant or variables not useful for the model such as the ones with not enough variability.

Starting with the dummy variables.

We will keep only dummies that has the event in at least 5% in the dataset:

```{r dummies_high, echo=TRUE}
dummy_variables <- dplyr::select(loan_dataset_logistic, 
                                 starts_with('x_client_gender'), 
                                 starts_with('x_district_name'),
                                 starts_with('x_region'),
                                 starts_with('x_card_type'))

dummy_variables_high <- tibble(variables = names(dummy_variables),
       zeros = sapply(dummy_variables, 
                      function(x) table(as.character(x) == 0)["TRUE"]),
       ones = sapply(dummy_variables, 
                     function(x) table(as.character(x) == 1)["TRUE"])) %>% 
  mutate(prop_ones = round(ones / (zeros + ones) * 100, 2)) %>% 
  arrange(prop_ones) %>% 
  filter(prop_ones  > 5)

kable(dummy_variables_high)
```


The remaining dummies will be excluded from the dataset as they do not have enough variability to fit a logistic model on them.

```{r dummies_reject, echo=FALSE}
dummy_variables_high <- dummy_variables_high$variables
dummy_variables_low <- names(dplyr::select(dummy_variables, -dummy_variables_high))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -dummy_variables_low)
```

After we investigated the low variability in the dummies we look at the calculated variables on the transaction type proportion we calculated dutring the data enhancement process.

```{r prop, echo=TRUE}
prop_variables <- dplyr::select(loan_dataset_logistic, 
                                starts_with('x_prop'))
prop_variables <- summary(prop_variables)
kable(t(prop_variables))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_prop_old_age_pension)
```

Variable **x_prop_old_age_pension** is also excluded from the data set as in the dataset no observation has this type of transaction.

We ended up having a data set with **40 variables**.

```{r variables_2, echo=FALSE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Investigating Multicollinearity

With the remaining variables we ran a multicollinearity test to identify additional variables to drop from the model specification.

```{r check_correl, out.width = '100%', warning=FALSE}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)
VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_Before <- tibble(variable = names(VIF$idiags[,1]),
                    VIF = VIF$idiags[,1]) %>% 
             arrange(desc(VIF))

knitr::kable(VIF_Table_Before)
```

Then we look at the correlogram in the high VIF variables.

```{r check_correl_2, out.width = '100%'}
low_VIF <- filter(VIF_Table_Before, VIF <= 5)$variable
high_VIF <- filter(VIF_Table_Before, VIF > 5)$variable

high_VIF_dataset <- dplyr::select(loan_dataset_logistic, high_VIF)

cor_mtx_high_VIF <- cor(high_VIF_dataset)

high_VIF_correlogram_before <- ggcorrplot(cor_mtx_high_VIF, 
                                hc.order = TRUE,
                                lab = FALSE,
                                lab_size = 3, 
                                method ="square",
                                colors = c("tomato2", "white", "springgreen3"),
                                title = "Correlation Matrix of Loan Dataset Variables with high VIF") +
  theme(axis.text = element_blank(),
        legend.position = 0)

print(high_VIF_correlogram_before)

```

Then we exclude the variables with high correlation keeping the variables with less correlation with the others.

```{r check_correl_3, out.width = '100%'}
correl_threshold <- 0.6

reject_variables_vector <- tibble(var_1 = row.names(cor_mtx_high_VIF)) %>% 
  bind_cols(as_tibble(cor_mtx_high_VIF)) %>% 
  melt(id = c("var_1")) %>% 
  filter(var_1 != variable) %>%
  mutate(abs_value = abs(value)) %>%
  filter(abs_value > correl_threshold) %>%
  group_by(var_1) %>% 
  mutate(sum_1 = sum(abs_value)) %>% 
  ungroup() %>% 
  group_by(variable) %>% 
  mutate(sum_2 = sum(abs_value)) %>% 
  ungroup() %>% 
  mutate(reject = ifelse(sum_1 > sum_2, var_1, as.character(variable))) %>% 
  distinct(reject)

reject_variables_vector <- reject_variables_vector$reject

clean_dataset <- dplyr::select(loan_dataset_logistic, -reject_variables_vector)

kable(reject_variables_vector)
```

We finally look on the complete dataset correlogram with the clean data set.

```{r check_correl_4, out.width = '100%'}
cor_mtx_full <- cor(loan_dataset_logistic)
cor_mtx_clean <- cor(clean_dataset)

full = ggcorrplot(cor_mtx_full, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Full Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

clean = ggcorrplot(cor_mtx_clean, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Clean Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

loan_dataset_logistic <- clean_dataset

print(ggarrange(full, clean))
```

We also look again at the VIF estimates for the clena dataset.

```{r check_correl_5, out.width = '100%'}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")
```

We finally texclude one last variable that still has a VIF higher than our threshold (5)

```{r check_correl_6, echo=FALSE, out.width = '100%'}
loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_average_salary)

vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")
```

After variable selection step we end up with **28 variables** in the dataset.

```{r variables_final, echo=TRUE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

```{r split_sample}
set.seed(12345)
index <- caret::createDataPartition(loan_dataset_logistic$y_loan_defaulter, 
                                    p= 0.7,list = FALSE)

data.train_logistic <- loan_dataset_logistic[index, ]
data.test_logistic  <- loan_dataset_logistic[-index,]

event_proportion <- bind_rows(prop.table(table(loan_dataset_logistic$y_loan_defaulter)),
                              prop.table(table(data.train_logistic$y_loan_defaulter)),
                              prop.table(table(data.test_logistic$y_loan_defaulter)))

event_proportion$scope = ''
event_proportion$scope[1] = 'full dataset'
event_proportion$scope[2] = 'train dataset'
event_proportion$scope[3] = 'test dataset'

event_proportion <- select(event_proportion, scope, everything())

kable(event_proportion)
```

Both datasets keep the same proportion for the explained variable around 11%.

## Fit the logistic model 

With the final cleaned dataset, we got from above steps we fit our Logistic Regression Y_loan_defaulter on all x variables.

```{r fit_model_full, echo=TRUE}
logistic.full <- glm(formula =y_loan_defaulter ~ .,
                     data= data.train_logistic, 
                     family= binomial(link='logit'))

names(logistic.full$coefficients) <- stringr::str_sub(names(logistic.full$coefficients), 1, 25)
summary(logistic.full)
```

Alternatively we fit a second model only with variables statistically significant p-value less than 10% using the stepwise method.

```{r fit_model_step_0, include=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
summary(logistic.step)
```

```{r fit_model_step_1, eval=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
summary(logistic.step)
```

```{r fit_model_step_show, echo=FALSE}
summary(logistic.step)
```

# Evaluating the model performance

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.


```{r predict, warning=FALSE}
logistic.prob.train <- predict(logistic.step, type = "response")
logistic.prob.test <- predict(logistic.step,
                              newdata = data.test_logistic, 
                              type= "response")
```

We then evaluate the metrics in the each model for Train and Test data:

```{r measure, warning=FALSE}

measures.logistic.train <- HMeasure(data.train_logistic$y_loan_defaulter, 
                                         logistic.prob.train, 
                                         threshold = 0.5)

measures.logistic.test <- HMeasure(data.test_logistic$y_loan_defaulter, 
                                        logistic.prob.test, 
                                        threshold = 0.5)

measures <- t(bind_rows(measures.logistic.train$metrics,
                        measures.logistic.test$metrics)) %>%
  as_tibble(., rownames = NA)

colnames(measures) <- c('logistic - train', 'logistic - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())

kable(measures, row.names = FALSE)

```

Then, we look a boxplot chart to see how well our model split the observation into our explained variable:

- **Logistic Regression**
```{r boxplot_2, out.width = '100%', echo=FALSE}
boxplot(logistic.prob.test ~ data.test_logistic$y_loan_defaulter,
        col= c("green", "red"), 
        horizontal= T,
        xlab = 'Probability Prediction',
        ylab = 'Loan Defaulter')
```

Then we plot the ROC(Receiver Operator Characteristic Curve) of the model:

```{r roC, out.width = '100%', echo=TRUE, warning=FALSE, message=FALSE}
roc_logistic <- roc(data.test_logistic$y_loan_defaulter,
                    logistic.prob.test)

y1 <- roc_logistic$sensitivities
x1 <- 1 - roc_logistic$specificities

plot(x1, y1,  type="n",
     xlab = "False Positive Rate (Specificities)", 
     ylab = "True Positive Rate (Sensitivities)")

lines(x1, y1, lwd = 3, lty = 1, col="red") 

legend("bottomright", c('Logistic'), 
       lty = 1, col = c('red'))

abline(0, 1, lty = 2)
```

Finally we look more closely to the model accuracy **Logistic Regression**.

To perform this task, we start by defining a threshold to assign the observation to each class, and them calculate the General Accuracy and the True Positive Rate.

- **Threshold = 0.1**

```{r accuracy, echo=TRUE}
accuracy <- function(score, actual, threshold = 0.5) {
  
  fitted.results <- ifelse(score > threshold ,1 ,0)
  
  misClasificError <- mean(fitted.results != actual)
  
  misClassCount <- misclassCounts(fitted.results, actual)

  print(kable(misClassCount$conf.matrix))

  print('--------------------------------------------------------------')
  print(paste('Model General Accuracy of: ', 
              round((1 - misClassCount$metrics['ER']) * 100, 2), '%', 
              sep = ''))
  print(paste('True Positive Rate of    : ', 
              round(misClassCount$metrics['TPR'] * 100, 2), '%',
              sep = ''))
}

accuracy(score = logistic.prob.test, 
         actual = data.test_logistic$y_loan_defaulter, 
         threshold = 0.1)

```
