---
title: "The logistic regression on loan's report"
date: "August, 2019"
---

```{r setup_logit, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```

# Objective

The goal of this session is trying to fit a Logistic Regression model on Loan data aiming to predict the probability of delinquency for each contract.

*******************************************************************************

# Modeling

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.


```{r variables, echo=TRUE}
loan_dataset_logistic <- source_dataset

kable(tibble(variables = names(loan_dataset_logistic)))
```

## Variable selection

Starting from this dataset we investigate the presence of redundant or variables not useful for the model such as the ones with not enough variability or with multicollinearity.

### Dummy variables

Starting with the dummy variables. We will keep only dummies that has the event in at least 5% of the observation in the dataset.

The below table shows the dummy variables that will be kept in the model:

```{r dummies_high, echo=TRUE}
dummy_variables <- dplyr::select(loan_dataset_logistic, 
                                 starts_with('x_client_gender'), 
                                 starts_with('x_district_name'),
                                 starts_with('x_region'),
                                 starts_with('x_card_type'))

dummy_variables_high <- tibble(variables = names(dummy_variables),
       zeros = sapply(dummy_variables, 
                      function(x) table(as.character(x) == 0)["TRUE"]),
       ones = sapply(dummy_variables, 
                     function(x) table(as.character(x) == 1)["TRUE"])) %>% 
  mutate(prop_ones = round(ones / (zeros + ones) * 100, 2)) %>% 
  arrange(prop_ones) %>% 
  filter(prop_ones  > 5)

kable(dummy_variables_high)
```

The remaining dummies will be excluded from the dataset as they do not have enough variability to fit a logistic model on them.

```{r dummies_reject, echo=FALSE}
dummy_variables_high <- dummy_variables_high$variables
dummy_variables_low <- names(dplyr::select(dummy_variables, -dummy_variables_high))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -dummy_variables_low)
```

### Transaction type proportion variables

After we investigated the low variability in the dummies, we take care of the calculated variables on the transaction type proportion we calculated during the data enhancement process.

```{r prop, echo=TRUE}
prop_variables <- dplyr::select(loan_dataset_logistic, 
                                starts_with('x_prop'))

prop_variables <- summary(prop_variables)

kable(t(prop_variables))

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_prop_old_age_pension)
```

The variable **x_prop_old_age_pension** is also excluded from the dataset as it has not a single observation in this sample.

We ended up having a data set with **40 variables**.

```{r variables_2, echo=FALSE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

### Multicollinearity on feature variables

Multicollinearity is the phenomenon in which a given predictor has a strong correlation with one or more predictors, in this scenario the multiple regression coefficient estimates may vary erratically depending on the data, or the model may not even be possible to be calculate in the case of a perfect correlation on the predictors.

So before we try to fit a model we ran a multicollinearity test to identify additional variables to drop from the model specification.

The idea here is to identify the variables with high correlation among them and keep just the variables that have no strong correlation to the other predictors. We will do so by preferring to keep the variables that have lowest correlation against the others.

Just looking at correlation indexes among pairs of predictors is limited as it is possible that the pairwise correlations are small, and yet a linear dependence exists among three or more variables.

That’s why we will rely on a test called Variance Inflation Factors, VIF for short, to detect multicollinearity.

Multicollinearity is poison for Regression algorithms!!!

```{r check_correl, out.width = '100%', warning=FALSE}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)
VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_Before <- tibble(variable = names(VIF$idiags[,1]),
                    VIF = VIF$idiags[,1]) %>% 
             arrange(desc(VIF))

knitr::kable(VIF_Table_Before)
```

Having identified the predictors with high VIF we look at the correlogram on those variables.

Here we are hiding the variable names for readability.

```{r check_correl_2, out.width = '100%'}
low_VIF <- filter(VIF_Table_Before, VIF <= 5)$variable
high_VIF <- filter(VIF_Table_Before, VIF > 5)$variable

high_VIF_dataset <- dplyr::select(loan_dataset_logistic, high_VIF)

cor_mtx_high_VIF <- cor(high_VIF_dataset)

high_VIF_correlogram_before <- ggcorrplot(cor_mtx_high_VIF, 
                                hc.order = TRUE,
                                lab = FALSE,
                                lab_size = 3, 
                                method="square",
                                colors = c("tomato2", "white", "springgreen3"),
                                title="Correlation Matrix of Loan Dataset Variables with high VIF") +
  theme(axis.text = element_blank(),
        legend.position = 0)

print(high_VIF_correlogram_before)

```

We will now exclude the variables with High VIF maintaining only the variables that are not correlated.

We will use the pairwise correlation indexes to choose the variable to reject, code below will exclude the variable in the pair that have the greater correlation index sum against the other predictor not in the pair.

At the end we will have only variables that have a correlation index smaller than our selected **threshold (0.6)**.

```{r check_correl_3, out.width = '100%'}
correl_threshold <- 0.6

reject_variables_vector <- tibble(var_1 = row.names(cor_mtx_high_VIF)) %>% 
  bind_cols(as_tibble(cor_mtx_high_VIF)) %>% 
  melt(id = c("var_1")) %>% 
  filter(var_1 != variable) %>%
  mutate(abs_value = abs(value)) %>%
  filter(abs_value > correl_threshold) %>%
  group_by(var_1) %>% 
  mutate(sum_1 = sum(abs_value)) %>% 
  ungroup() %>% 
  group_by(variable) %>% 
  mutate(sum_2 = sum(abs_value)) %>% 
  ungroup() %>% 
  mutate(reject = ifelse(sum_1 > sum_2, var_1, as.character(variable))) %>% 
  distinct(reject)

reject_variables_vector <- reject_variables_vector$reject

clean_dataset <- dplyr::select(loan_dataset_logistic, -reject_variables_vector)

kable(reject_variables_vector)
```

After the high VIF predictors exclusion we reevaluate the correlation matrix of the original dataset and the cleaned dataset.

Variable names are hidden for readability.
Trust us, there is no correlation greater than 0.6 here.

```{r check_correl_4, out.width = '100%'}
cor_mtx_full <- cor(loan_dataset_logistic)
cor_mtx_clean <- cor(clean_dataset,)

full = ggcorrplot(cor_mtx_full, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Full Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

clean = ggcorrplot(cor_mtx_clean, hc.order = TRUE,
           lab = FALSE, 
           lab_size = 3, 
           method="square", 
           colors = c("tomato2", "white", "springgreen3"),
           title="Correlation Matrix of Clean Loan Dataset") +
  theme(axis.text = element_blank(),
        legend.position = 0)

print(ggarrange(full, clean))

loan_dataset_logistic <- clean_dataset
```

Once again let’s look on the VIF estimates.

```{r check_correl_5, out.width = '100%'}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")

loan_dataset_logistic <- dplyr::select(loan_dataset_logistic, -x_average_salary)
```

Although **x_average_salary** have no correlation index greater than our defined **threshold (0.6)** it still have a VIF greater than 5, and for pure methodological puritanism we decided to exclude this variable as well.

We did try to keep it, but it had no real difference.

```{r check_correl_6, echo=FALSE, out.width = '100%'}
vars.quant <- select_if(loan_dataset_logistic, is.numeric)

VIF <- imcdiag(vars.quant, loan_dataset_logistic$y_loan_defaulter)

VIF_Table_After <- tibble(variable = names(VIF$idiags[,1]),
                          VIF = VIF$idiags[,1]) %>%
  arrange(desc(VIF))

ggplot(VIF_Table_After, aes(x = fct_reorder(variable, VIF), 
                            y = log(VIF), label = round(VIF, 2))) + 
  geom_point(stat='identity', fill="black", size=15)  +
  geom_segment(aes(y = 0, 
                   yend = log(VIF), 
                   xend = variable), 
               color = "black") +
  geom_text(color="white", size=4) +
  geom_hline(aes(yintercept = log(5)), color = 'red', size = 2) +
  scale_y_continuous(labels = NULL, breaks = NULL) +
  coord_flip() +
  theme_economist() +
  theme(legend.position = 'none', 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
  labs(x = 'Variable',
       y = NULL,
       title = 'Variance Inflation Factor',
       subtitle="Checking for multicolinearity in X's variables.
       Variables with VIF more than 5 will be droped from the model")
```

After all this work we end up having **28 variables** in the dataset.

```{r variables_final, echo=TRUE}
kable(tibble(variables = names(loan_dataset_logistic)))
```

## Looking for outliers

And what about outliers?

Regression algorithms are known for its sensitivity for outliers. So, before we go for the modeling juice, we will look for those bastards!!!

Looking at the plots we see no significant deviation in the quantitative variables.

**x_last_transaction_age_days** looks to have a discrepancy, but it is ranging from 0 to 30 days, so it is not real outliers.

```{r outliers, echo=TRUE, out.width = '100%'}
# outliers ----------------------------------------------------------------------------
  attach(loan_dataset_logistic)

  par(mfrow=c(2, 3))

  plot(x_last_transaction_age_days, main = "x_last_transaction_age_days", ylab = '')
  plot(x_avg_account_balance, main = "x_avg_account_balance", ylab = '')
  plot(x_account_balance, main = "x_account_balance", ylab = '')
  plot(x_card_age_month, main = "x_card_age_month", ylab = '')
  plot(x_client_age, main = "x_client_age", ylab = '')
  plot(x_loan_amount, main = "x_loan_amount", ylab = '')
  
  par(mfrow=c(2, 3))
  
  plot(x_loan_duration, main = "x_loan_duration", ylab = '')
  plot(x_loan_payments, main = "x_loan_payments", ylab = '')
  plot(x_prop_interest_credited, main = "x_prop_interest_credited", ylab = '')
  plot(x_prop_loan_payment, main = "x_prop_loan_payment", ylab = '')
  plot(x_prop_statement, main = "x_prop_statement", ylab = '')
  plot(x_prop_insurance_payment, main = "x_prop_insurance_payment", ylab = '')

  detach(loan_dataset_logistic)
```


## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

```{r split_sample}
SplitDataset <- source_train_test_dataset
data.train_logistic <- SplitDataset$data.train
data.test_logistic <- SplitDataset$data.test

data.train_logistic <- dplyr::select(data.train_logistic, names(loan_dataset_logistic))
data.test_logistic <- dplyr::select(data.test_logistic, names(loan_dataset_logistic))

kable(SplitDataset$event.proportion)
```

Both datasets keep the same proportion for the explained variable around 11%.

This split was saved in disk using the function described in the Data Prep session to be reused in the other models fitted in this exercise.

This will ensure consistency when comparing the models against each other.

## Fit the logistic model 

With the final cleaned dataset, we got from above steps we fit our Logistic Regression model for **y_loan_defaulter** on all **x_variables**.

```{r fit_model_full, echo=TRUE}
logistic.full <- glm(formula = y_loan_defaulter ~ .,
                     data= data.train_logistic, 
                     family= binomial(link='logit'))

names(logistic.full$coefficients) <- stringr::str_sub(names(logistic.full$coefficients), 1, 25)
summary(logistic.full)
```

With the full model fitted we will run the stepwise method to automate the selection of predictors using the **(Akaike Information Criterion)** AIC for short.

This is a broadly used estimator that evaluates the relative quality of statistical methods for a given set of data.

The lower the AIC the better.

```{r fit_model_step_0, include=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
summary(logistic.step)
```

```{r fit_model_step_1, eval=FALSE}
logistic.step <- step(logistic.full, direction = "both", test = "F")

names(logistic.step$coefficients) <- stringr::str_sub(names(logistic.step$coefficients), 1, 25)
```

```{r fit_model_step_show, echo=FALSE}
summary(logistic.step)
```

*******************************************************************************

# Interpreting model output

Logistic Regression models are not only good for prediction but also for inference.

Meaning that it can be used not only in predicting the classification into Defaulters and Non-Defaulters but also to understand the relationship between the predictors and the likelihood of delinquency.

In the Logistic Regression the effect of an increase or decrease of a predictor is not linear, but we can see the predictors that have the greater influence and by its coefficient signal the associated with an increase or decrease on the likelihood of default.

The probability of default is given by below equation:

$$P(Y = 1 | X_{n} = x_{n}) = \frac{1}{1 + e^{(-\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+...+\beta_{n}x_{n} )}}$$

Where each β is the coefficient estimate for each variable in the final model.

In our model we see that below variables are associated to an increase in the likelihood of default:

- **x_loan_amount**
- **x_prop_interest_credited**
- **x_prop_statement**

On the other hand, below variables are associated to a decrease in the likelihood of default:

- **x_card_type_classic**
- **x_loan_duration**
- **x_no_of_commited_crimes_1995**
- **x_loan_amount**
- **x_region_north_Bohemia**

*******************************************************************************

# Evaluating the model performance

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.

```{r get_scores, echo=TRUE}
## making predictions for each model and consolidating in a single data frame

prob.full   = list()
prob.train  = list()
prob.test   = list()

prob.full$logistic.actual     <- loan_dataset_logistic$y_loan_defaulter
prob.full$logistic.predicted  <- predict(logistic.step, type = "response", 
                                         newdata = loan_dataset_logistic)

prob.train$logistic.actual    <- data.train_logistic$y_loan_defaulter
prob.train$logistic.predicted <- predict(logistic.step, type = "response", 
                                         newdata = data.train_logistic)

prob.test$logistic.actual     <- data.test_logistic$y_loan_defaulter
prob.test$logistic.predicted  <- predict(logistic.step, type = "response", 
                                         newdata = data.test_logistic)

prob.full   <- prob.full  %>% as_tibble()
prob.train  <- prob.train %>% as_tibble()
prob.test   <- prob.test  %>% as_tibble()
```

## Getting Performance Measures

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

We will use the custom functions described in Auxiliary metrics functions topic in the Data Preparation session of this site.

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------

metricsByCutoff.test_log    <- modelMetrics(prob.test$logistic.actual, 
                                            prob.test$logistic.predicted, 
                                            plot_title = 'Logistic Regression')
metricsByCutoff.test_log$Plot
```

With the optimized cut off we calculate the full set of model metrics using the function HMeasure from hmeasure library.

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# logistic regression
measures.logistic.train <- HMeasure(prob.train$logistic.actual, 
                                    prob.train$logistic.predicted, 
                                    threshold = metricsByCutoff.test_log$BestCut['Cut'])
measures.logistic.test <- HMeasure(prob.test$logistic.actual, 
                                   prob.test$logistic.predicted, 
                                   threshold = metricsByCutoff.test_log$BestCut['Cut'])

# join measures in a single data frame
measures <- t(bind_rows(measures.logistic.train$metrics,
                        measures.logistic.test$metrics)
              ) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('logistic - train', 'logistic - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())

```

Below are the metrics on the train and test dataset:
```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

## Evaluating classification performance

In general the Logistic Regression model is not delivering good accuracy, we will compare how it performed against other classes of models in the Final Report session

Below the confusion matrix and general performance of the model using our custom function **accuracy** described in Auxiliary metrics functions topic in the Data Preparation session of this site.

```{r see_accuracy_1, echo=TRUE, out.width= '100%'}
# accuracy metrics ---------------------------------------------------------------
# logistic regression
accuracy(score = prob.test$logistic.predicted, 
         actual = prob.test$logistic.actual, 
         threshold = metricsByCutoff.test_log[["BestCut"]][["Cut"]])
```

We finally look at the score distribution charts to check how well the model is able to discriminate Defaulters and Non-Defaulters.

```{r model_plots, echo=TRUE, out.width= '100%'}
p1 <- Score_Histograms(prob.test, 
                 prob.test$logistic.predicted,
                 prob.test$logistic.actual,
                 'Density Plot') + theme(axis.title.y = element_blank())

p2 <- Score_Boxplot(prob.test, 
              prob.test$logistic.predicted, 
              prob.test$logistic.actual,
              'Score Boxplot')

p3 <- KS_Plot(prob.test$logistic.predicted[prob.test$logistic.actual == 0],
        prob.test$logistic.predicted[prob.test$logistic.actual == 1],
        'KS Plot') + theme(axis.title.y = element_blank(),
                           axis.text.y = element_blank())
ggarrange(p1, p2, nrow = 2)

p3
```

By the score density we see that our Logistic Regression model is very broad on the scores it assigns to the observations, especially for the real Defaulters not providing a good separation between each class.

The box plot also does not show a clear discrimination between Defaulters and Non-Defaulters.

Finally, the KS metric is far from what is expected for a reasonable classification model.

In the Final Report session, we will look more closely on the AUC and Gini metrics by plotting the ROC curve and comparing against other models.