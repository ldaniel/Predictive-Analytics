---
title: "The random forest on loan's report"
date: "August, 2019"
---

```{r setup_rf, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)
library(tufte)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```


# Objective

The goal of this session is to fit a Random Forest model on Loan data aiming to predict the probability of delinquency for each contract.

Random forest, in essence, consists of a large set of individual decision trees operating as an ensemble. Therefore, each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.

> The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is: A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.
> `r tufte::quote_footer('--- Tony Yiu')`

The RF algorithm is a supervised algorithm that, even though it can be used for regression purposes, it was initially conceived as a classification tool.

The method follows the same concept as a decision tree but with the power of the crowd. So, basically, instead of using one big, deep and complex tree, the method believes that multiple randomly different (in multiple ways) trees can perform better, even if individually the trees are not very good classifiers.


*******************************************************************************

# Modeling

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.


```{r variables, echo=TRUE}
loan_dataset_rf <- source_dataset

kable(tibble(variables = names(loan_dataset_rf)))
```

## Variable selection

One advantage of Random Forest models is that it does not require heavy feature engineering.

We will only remove **x_prop_old_age_pension** that we know beforehand to have no variance in the dataset.

This model is also not sensible to outliers, missing values and multicollinearity.


```{r variable_selecion, echo=TRUE}
loan_dataset_rf <- dplyr::select(loan_dataset_rf, -x_prop_old_age_pension)
```

## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

We are selecting exact the same samples for all models to allow comparison between then.

```{r sampling, echo=TRUE}
SplitDataset <- source_train_test_dataset
data.train_rf <- SplitDataset$data.train
data.test_rf <- SplitDataset$data.test

kable(SplitDataset$event.proportion)

loan_dataset_rf$y_loan_defaulter <- as.factor(loan_dataset_rf$y_loan_defaulter)
data.train_rf$y_loan_defaulter   <- as.factor(data.train_rf$y_loan_defaulter)
data.test_rf$y_loan_defaulter    <- as.factor(data.test_rf$y_loan_defaulter)

data.train_rf <- dplyr::select(data.train_rf, names(loan_dataset_rf))
data.test_rf <- dplyr::select(data.test_rf, names(loan_dataset_rf))
```

Both datasets keep the same proportion for the explained variable around 11%.

*******************************************************************************

# Selecting the best parameters values for the Random Forest

The R community is a one of R's best features. There are many community members doing awesome improvements on the existent libraries. 

A good example is Jason Brownlee, who extended **Caret** library by adding a new approach that supports multiple tuning of multiple parameters. Please, see [**References**](https://ldaniel.github.io/Predictive-Analytics/09_references.html) page for further details and links.

He defined his own algorithm to use in caret by defining a list that contains a number of custom named elements that the caret package looks for, such as how to fit and how to predict. See below for a definition of a custom random forest algorithm for use with caret that takes both an mtry and ntree parameters.

```{r extent_caret, echo=TRUE, eval=FALSE}
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)

customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), 
                                  class = rep("numeric", 2), 
                                  label = c("mtry", "ntree"))

customRF$grid <- function(x, y, len = NULL, search = "grid") {}

customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}

customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)

customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")

customRF$sort <- function(x) x[order(x[,1]),]

customRF$levels <- function(x) x$classes
```

The following code is able to fit the random forest model using the caret customized train function above. Then, we just have to give some parameters values to be tested and after running this pretty code we can benefits from its results by having which parameters performed best.

```{r select_best_parameters, echo=TRUE, eval=FALSE}
control <- trainControl(method="repeatedcv", 
                        number=5, 
                        repeats=3, 
                        verboseIter = TRUE, 
                        allowParallel = TRUE)

tuneparam <- expand.grid(.mtry=c(5, 25, 50, 75, 85, 100, 115, 125, 150, 175, 200),
                         .ntree=c(1000, 3000, 5000, 7000, 9000, 10000))

evalmetric <- "Accuracy"

set.seed(12345)

ini <- Sys.time()
cat(paste0("\nStarted RF training at: ", ini, " ...\n\n"))

rf.full <- train(y_loan_defaulter ~ .,
                 data=data.train_rf,
                 method=customRF,
                 metric=evalmetric,
                 tuneGrid=tuneparam,
                 trControl=control,
                 importance=TRUE)

elapsedTime <- difftime(Sys.time(), ini, units = "auto")
cat(paste0("\n\nFinished RF training. Total time taken: ", round(elapsedTime, 2), " ", units(elapsedTime)))

summary(rf.full)
plot(rf.full)
```

After some time waiting for the results, about 5 hours, we ended up selecting the best parameters, as following: 

- **mtry = 85** 
- **ntree = 3000**

Last but not least, we saved the final model results on disk to be quickly consumed when necessary.

```{r save_rds_file, echo=TRUE, eval=FALSE}
saveRDS(rf.full, "./models/random_forest.rds")
```

So, to save time, we only have to load the fitted model saved on disk.

```{r read_rds_file, echo=TRUE}
rf.full <- readRDS("./models/random_forest.rds")
```

*******************************************************************************

# Interpreting model output

For this model the four principal vars are:

- **x_prop_interest_credited**
- **x_account_balance**
- **x_avg_account_balance**
- **x_loan_amount**

The result is similar to the Logistic Regression, Decision Tree and Boosting models.

*******************************************************************************

# Evaluating the model performance

Here we will perform basically the same steps we did in the Logistic Regression, Decision Tree and Boosting models.

A comparison against all the models will be provided in the Final Report session of this exercise.

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.


```{r get_scores, echo=TRUE, out.width= '100%'}
## making preditions for each model and consolidating in a single data frame

prob.full  = list()
prob.train = list()
prob.test  = list()

prob.full$randomforest.actual     <- loan_dataset_rf$y_loan_defaulter
prob.full$randomforest.predicted  <- predict(rf.full, newdata = loan_dataset_rf, 
                                             type = "prob")[,2]

prob.train$randomforest.actual    <- data.train_rf$y_loan_defaulter
prob.train$randomforest.predicted <- predict(rf.full, newdata = data.train_rf, 
                                             type = "prob")[,2]

prob.test$randomforest.actual     <- data.test_rf$y_loan_defaulter
prob.test$randomforest.predicted  <- predict(rf.full, newdata = data.test_rf, 
                                             type = "prob")[,2]

prob.full  <- prob.full %>% as_tibble()
prob.train <- prob.train %>% as_tibble()
prob.test  <- prob.test %>% as_tibble()
```


## Getting Performance Measures

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------
metricsByCutoff.test_randomforest  <- modelMetrics(prob.test$randomforest.actual, 
                                                   prob.test$randomforest.predicted, 
                                                   plot_title = 'Random Forest')
metricsByCutoff.test_randomforest$Plot
```

With the optimized cut off we calculate the full set of model metrics using the function HMeasure from hmeasure library.

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# Random Forest
measures.randomforest.train <- HMeasure(prob.train$randomforest.actual, 
                                        prob.train$randomforest.predicted, 
                                        threshold = metricsByCutoff.test_randomforest$BestCut['Cut'])
measures.randomforest.test  <- HMeasure(prob.test$randomforest.actual, 
                                        prob.test$randomforest.predicted, 
                                        threshold = metricsByCutoff.test_randomforest$BestCut['Cut'])


# join measures in a single data frame
measures <- t(bind_rows(measures.randomforest.train$metrics,
                        measures.randomforest.test$metrics
                        )) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('random forest - train', 'random forest - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())
```

Below are the metrics on the train and test dataset:

```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

## Evaluating classification performance

To-do.