---
title: "The random forest on loan's report"
date: "August, 2019"
---

```{r setup_rf, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)
library(tufte)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```


# Objective

The goal of this session is to fit a Random Forest model on Loan data aiming to predict the probability of delinquency for each contract.

Random forest, in essence, consists of a large set of individual decision trees operating as an ensemble. Therefore, each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.

> The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is: A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.
> `r tufte::quote_footer('--- Tony Yiu')`

The Random Forest algorithm is a supervised algorithm that, even though it can be used for regression purposes, it was initially conceived as a classification tool.

The method follows the same concept as a decision tree but with the power of the crowd. So, basically, instead of using one big, deep and complex tree, the method relies on multiple randomly different (in multiple ways) trees voting for a class. This method usually perform better better than one very well trained tree even if, individually, the trees are not as good classifiers, thus refering to the forementioned wisdom of the crowds.

>Random forest thrives even in scenarios, when there is an abundance of chaos, i.e. many predictors. It’s hard to know which predictor is important and which is not. All the other traditional statistical techniques might fail or struggle when we have an incredibly high number of independent variables.
> `r tufte::quote_footer('--- Pranov Mishra')`

Because of the voting system inherent to the method it is often said to be democratic algorithm.

>Now, if you think for a second, this is the way direct democracy works: each voter has access to a subset of the information and only sees that subset from a particular perspective (their own unique perspective). By using a majority vote, we are actually implementing a Random Forest.
> `r tufte::quote_footer('--- Pablo Duboue')`


*******************************************************************************

# Modeling

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a dataset with **118 variables**.


```{r variables, echo=TRUE}
loan_dataset_rf <- source_dataset

kable(tibble(variables = names(loan_dataset_rf)))
```

## Variable selection

One advantage of Random Forest models is that it does not require heavy feature engineering.

We will only remove **x_prop_old_age_pension** that we know beforehand to have no variance in the dataset.

Mainly beacause of the ramdomness in variable selection of random forest algorithm, this model is not sensible to outliers, missing values and multicollinearity.


```{r variable_selecion, echo=TRUE}
loan_dataset_rf <- dplyr::select(loan_dataset_rf, -x_prop_old_age_pension)
```

## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

We are selecting exact the same samples for all models to allow comparison between then.

```{r sampling, echo=TRUE}
SplitDataset <- source_train_test_dataset
data.train_rf <- SplitDataset$data.train
data.test_rf <- SplitDataset$data.test

kable(SplitDataset$event.proportion)

loan_dataset_rf$y_loan_defaulter <- as.factor(loan_dataset_rf$y_loan_defaulter)
data.train_rf$y_loan_defaulter   <- as.factor(data.train_rf$y_loan_defaulter)
data.test_rf$y_loan_defaulter    <- as.factor(data.test_rf$y_loan_defaulter)

data.train_rf <- dplyr::select(data.train_rf, names(loan_dataset_rf))
data.test_rf <- dplyr::select(data.test_rf, names(loan_dataset_rf))
```

Both datasets kept the same proportion for the explained variable at about 11%.

*******************************************************************************

# Selecting the best parameters values for the Random Forest

The R community is a one of R's best features. There are many community members doing awesome improvements on existent libraries as well as sharing and spreading knowledge to the four corners of the Earth (if you still think the Earth is flat and square as I do).

Th algorithm selected from **randomForest package** (Please, see [**References**](https://ldaniel.github.io/Predictive-Analytics/09_references.html) to reach out this amazing package) have two main parameters for random forest algorithm tuning: **mtry** representing the number of variables randomly sampled as candidates at each split (or the size of the trees) and **ntree** representing the number of trees to grow (or the size of the forest).

Another great package for modeling is caret (*seriously*, check this one out in [**References**](https://ldaniel.github.io/Predictive-Analytics/09_references.html), it is fantastic in so many ways!) and, for the sake of our sanity, it has already implemented a good method for randomForest's package parameter tuning (cheers to them!). This, with the expanded grid search (also from caret package), provides a high performance, friendly and intelligent way of testing parameters. Unfortunately, it is not a perfect world and the implemented method only tune **mtry** parameter.

In order to have both parameters tuned using expanded grid search we need to extend caret's methodology, creating a custom method for parameter tuning and, for that, even under the risk of being repetitive, we must say: we can be saved by the grace of of R's community!

There are a lot of implementations accross the internet, however, a good example (and apparently an original one) is provided by Jason Brownlee, who created this customized function supports **mtry** AND **ntree** parameter tuning together. Please, see [**References**](https://ldaniel.github.io/Predictive-Analytics/09_references.html) page for Jason's credits (cheers to him!).

Below is his implementation to be further used in our RF training.

```{r extent_caret, echo=TRUE, eval=FALSE}
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)

customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), 
                                  class = rep("numeric", 2), 
                                  label = c("mtry", "ntree"))

customRF$grid <- function(x, y, len = NULL, search = "grid") {}

customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}

customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)

customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")

customRF$sort <- function(x) x[order(x[,1]),]

customRF$levels <- function(x) x$classes
```

Moving further, we can now set up our expanded grid search using our customized RF train method to test multiple parameters and its different combinations. After that, caret's method will automatically select best model according to the metric we defined (in our case, accuracy).
Moreover, to provide a reliable method of training to avoid (as well as we can) overfitting, we used repeated k-fold cross validation as train control - also provided by caret's package (we warned you, this is, indeed, an amazing package!).

```{r select_best_parameters, echo=TRUE, eval=FALSE}
control <- trainControl(method="repeatedcv", 
                        number=5, 
                        repeats=3, 
                        verboseIter = TRUE, 
                        allowParallel = TRUE)

tuneparam <- expand.grid(.mtry=c(5, 25, 50, 75, 85, 100, 115, 125, 150, 175, 200),
                         .ntree=c(1000, 3000, 5000, 7000, 9000, 10000))

evalmetric <- "Accuracy"

set.seed(12345)

ini <- Sys.time()
cat(paste0("\nStarted RF training at: ", ini, " ...\n\n"))

rf.full <- train(y_loan_defaulter ~ .,
                 data=data.train_rf,
                 method=customRF,
                 metric=evalmetric,
                 tuneGrid=tuneparam,
                 trControl=control,
                 importance=TRUE)

elapsedTime <- difftime(Sys.time(), ini, units = "auto")
cat(paste0("\n\nFinished RF training. Total time taken: ", round(elapsedTime, 2), " ", units(elapsedTime)))

summary(rf.full)
plot(rf.full)
```

After some cups of coffee (and maybe some time spent on your preferred streaming provider), we have the training finished. And the winners are...: 

- **mtry = 85** 
- **ntree = 3000**

Last but not least, we saved the final model results on disk to be quickly consumed when necessary.

```{r save_rds_file, echo=TRUE, eval=FALSE}
saveRDS(rf.full, "./models/random_forest.rds")
```

So, to save time, we only have to load the fitted model saved on disk.

```{r read_rds_file, echo=TRUE}
rf.full <- readRDS("./models/random_forest.rds")
```

*******************************************************************************

# Interpreting model output

For this model the four principal vars are:

- **x_prop_interest_credited**
- **x_account_balance**
- **x_avg_account_balance**
- **x_loan_amount**

The result is similar to the Logistic Regression, Decision Tree and Boosting models.

*******************************************************************************

# Evaluating the model performance

Here we will perform basically the same steps we did in the Logistic Regression, Decision Tree and Boosting models.

A comparison against all the models will be provided in the Final Report session of this exercise.

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.


```{r get_scores, echo=TRUE, out.width= '100%'}
## making preditions for each model and consolidating in a single data frame

prob.full  = list()
prob.train = list()
prob.test  = list()

prob.full$randomforest.actual     <- loan_dataset_rf$y_loan_defaulter
prob.full$randomforest.predicted  <- predict(rf.full, newdata = loan_dataset_rf, 
                                             type = "prob")[,2]

prob.train$randomforest.actual    <- data.train_rf$y_loan_defaulter
prob.train$randomforest.predicted <- predict(rf.full, newdata = data.train_rf, 
                                             type = "prob")[,2]

prob.test$randomforest.actual     <- data.test_rf$y_loan_defaulter
prob.test$randomforest.predicted  <- predict(rf.full, newdata = data.test_rf, 
                                             type = "prob")[,2]

prob.full  <- prob.full %>% as_tibble()
prob.train <- prob.train %>% as_tibble()
prob.test  <- prob.test %>% as_tibble()
```


## Getting Performance Measures

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------
metricsByCutoff.test_randomforest  <- modelMetrics(prob.test$randomforest.actual, 
                                                   prob.test$randomforest.predicted, 
                                                   plot_title = 'Random Forest')
metricsByCutoff.test_randomforest$Plot
```

With the optimized cut off we calculate the full set of model metrics using the function HMeasure from hmeasure library (another very good package! Don't forget to check for our [**references**](https://ldaniel.github.io/Predictive-Analytics/09_references.html)).

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# Random Forest
measures.randomforest.train <- HMeasure(prob.train$randomforest.actual, 
                                        prob.train$randomforest.predicted, 
                                        threshold = metricsByCutoff.test_randomforest$BestCut['Cut'])
measures.randomforest.test  <- HMeasure(prob.test$randomforest.actual, 
                                        prob.test$randomforest.predicted, 
                                        threshold = metricsByCutoff.test_randomforest$BestCut['Cut'])


# join measures in a single data frame
measures <- t(bind_rows(measures.randomforest.train$metrics,
                        measures.randomforest.test$metrics
                        )) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('random forest - train', 'random forest - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())
```

Below are the metrics on the train and test dataset:

```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

Our Random Forest model clearly overfitted (as we can see by most of metrics, AUC of 1.0 in train set and 0.84 in test set, for example). This happened mainly because of the size of our dataset.

## Evaluating classification performance

This model delivered an amazing result but not the best one (beaten by the boosting model). We have a full session to compare how it performed against other models in the Final Report session. But wait, you should't hurry, there are still some steps to evaluate RF algorithm.

Below the confusion matrix and general performance of the model:

```{r see_accuracy_1, echo=TRUE, out.width= '100%'}
# accuracy metrics ---------------------------------------------------------------
# random forest
accuracy(score = prob.test$randomforest.predicted, 
         actual = prob.test$randomforest.actual, 
         threshold = metricsByCutoff.test_randomforest[["BestCut"]][["Cut"]])
```

We finally look at the score distribution charts to check how well the model is able to discriminate Defaulters and Non-Defaulters.

```{r model_plots, echo=TRUE, out.width= '100%'}
p1 <- Score_Histograms(prob.test, 
                 prob.test$randomforest.predicted,
                 prob.test$randomforest.actual,
                 'Density Plot') + theme(axis.title.y = element_blank())

p2 <- Score_Boxplot(prob.test, 
              prob.test$randomforest.predicted, 
              prob.test$randomforest.actual,
              'Score Boxplot')

p3 <-   KS_Plot(prob.test$randomforest.predicted[prob.test$randomforest.actual == 0],
          prob.test$randomforest.predicted[prob.test$randomforest.actual == 1],
          'Random Forest')

ggarrange(p1, p2, nrow = 2)

p3
```

By the score density we see that our Random Forest model provides a narrow and precise discrimination around defaulters.

The box plot also shows a clear discrimination between Defaulters and Non-Defaulters.

Finally, the KS metric is also presented a good result for a reliable classification purpose.

In the Final Report session, we will look more closely on the AUC and Gini metrics by plotting the ROC curve and comparing against other models.

Stay tuned!!!
