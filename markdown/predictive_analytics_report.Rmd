---
title: "FGV MBA - Trabalho de AnÃ¡lise Preditiva"
author:
- Daniel Campos (A99999999 / nome@email.com)
- Leandro Daniel (A57622988 / ldanielcontato@gmail.com)
- Rodrigo Goncalves (A99999999 / nome@email.com)
- Ygor Lima (A99999999 / nome@email.com)
date: "Julho de 2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries
library(dplyr)
library(ggplot2)
library(tidyr)
library(lubridate)
library(ggalluvial)
library(stringr)
library(VIM)
library(psych)
library(ggthemes)
library(tinytex)
```

```{r include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
#source("./scripts/step_03_data_cleaning.R")
#TO-DO: to discuss with team how to handle data cleaning
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
```

# setting the scene

## Domain
To-do.

## Task descripion

To-do.

## Data description

To-do.

## Project at GitHub
This project can be found and downloaded at GitHub: https://github.com/ldaniel/Predictive-Analytics

> Valar Morghulis!
> :)

*******************************************************************************

# Data ingestion, cleaning, translation and enhancement

Before starting the Berka Analysis, a few important steps were taken in order to prepare the source data files. These steps are listed below:

- **Step 01**: Create Functions;
- **Step 02**: Data Ingestion;
- **Step 03**: Data Cleaning;
- **Step 04**: Label Translation;
- **Step 05**: Data Enhancement.

## Create Functions
This step create functions to be used in the next steps. Following, all functions created were described.

### Function name
To-do.

```
DummyFunction <- function(var) {
  return(var)
}
```

## Data Ingestion
During this step, in addition to the loading data processes, it was performed data casting, column renaming and small touch-ups. The list below describe each table adjustment taken:

- **Table**: To-do.

## Data Cleaning

The objective of this step was analysing missing values and other strange conditions. In order to accomplish this task, a few R functions were used to quickly discover missing values, like NA and empty fields.

The following command were used in each table to find out where NA values. 

```
sapply(table, function(x) sum(is.na(x)))
```
Solely the <NAME> table has 760931 NA's in the account column.

```
sapply(table, function(x) table(as.character(x) =="")["TRUE"])
```
Again, only the <NAME> table has empty values, in the following columns:

- Column1: 183114 empty cells;
- Column2: 481881 empty cells;
- Column3: 782812 empty cells.

## Label Translation
In order to make the data information more understandable, it was translated some relevant labels and domains from Czech to English.

## Data Enhancement
This step aims to improve the analysis by adding auxiliary information.

The code below improved loan data by having a classification regarding its payment status.

```
loan <- mutate(loan, defaulter = 
                as.logical(plyr::mapvalues(status, c ('A','B','C','D'), 
                                           c(FALSE,TRUE,FALSE,TRUE))),
                contract_status = plyr::mapvalues(status, c ('A','B','C','D'), 
                                 c('finished','finished','running','running')),
                                 type = 'Owner')
```

The code below improved client data by havivng its age group.

```
client <- mutate(client, age_bin = paste(findInterval(age, 
                 c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)) * 10,'+'))
```

*******************************************************************************

# The Analysis

## Table Exploration

To-do.

```{r gender_distribution_analysis, echo=TRUE, out.width = '100%'}
# To-do
```

