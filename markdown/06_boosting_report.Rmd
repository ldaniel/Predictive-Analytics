---
title: "The boosting on loan's report"
date: "August, 2019"
---

```{r setup_logit, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos =  "h")
knitr::opts_knit$set(root.dir = "../")

# loading required libraries ----------------------------------------------------------
# libraries for data prep
library(dplyr)
library(readr)
library(magrittr)
library(forcats)
library(lubridate)
library(stringr)
library(feather)
library(fastDummies)
library(reshape2)
library(knitr)

#libraries for plots
library(ggplot2)
library(ggthemes)
library(ggcorrplot)
library(ggpubr)
library(plotly)

# libraries for data clean
library(VIM)
library(rms)
library(mctest)

# libraries for modeling
library(caret)
library(gmodels)
library(MASS)
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)

# libraries for measures
library(hmeasure)
library(pROC)

```

```{r scripts, include=FALSE}
# loading required steps before performing the analysis
source("./scripts/step_01_create_functions.R")
source("./scripts/step_02_data_ingestion.R")
source("./scripts/step_03_data_cleaning.R")
source("./scripts/step_04_label_translation.R")
source("./scripts/step_05_data_enhancement.R")
source("./scripts/step_06_dataset_preparation.R")
```

# Objective

The goal of this session is trying to fit a Boosting model on Loan data aiming to predict the probability of delinquency for each contract.

*******************************************************************************

# Modeling

## Dataset preparation

Using the vanilla transaction dataset, we calculated several derived variables for each account as described in the Data Preparation session.

This dataset is joined with Loan, Client, Credit Card, District, Account and Account Balance tables.

We ended up having a data set with **118 variables**.


```{r variables, echo=TRUE}

loan_dataset_boost <- source_dataset

kable(tibble(variables = names(loan_dataset_boost)))

```


## Variable selection

One advantage of Boosting models is that it does not require heavy feature engineering.

We will only remove **x_prop_old_age_pension** that we know beforehand to have no variance in the dataset.

This model is also not sensible to outliers, missing values and multicollinearity.


```{r variable_selecion, echo=TRUE}

loan_dataset_boost <- dplyr::select(loan_dataset_boost, -x_prop_old_age_pension)

```


## Sample split into Test and Training Data

The available data in Loan Dataset is split into Train and Testing data on the following proportion:

- **Train Dataset** (70% 478 obs);
- **Test Dataset ** (30% 204 obs).

We are selecting exact the same samples for all models to allow comparison between then.

```{r sampling, echo=TRUE}

SplitDataset <- source_train_test_dataset
data.train_boost <- SplitDataset$data.train
data.test_boost <- SplitDataset$data.test

kable(SplitDataset$event.proportion)

loan_dataset_boost$y_loan_defaulter <- as.factor(loan_dataset_boost$y_loan_defaulter)
data.train_boost$y_loan_defaulter   <- as.factor(data.train_boost$y_loan_defaulter)
data.test_boost$y_loan_defaulter    <- as.factor(data.test_boost$y_loan_defaulter)

data.train_boost <- dplyr::select(data.train_boost, names(loan_dataset_boost))
data.test_boost <- dplyr::select(data.test_boost, names(loan_dataset_boost))

```

Both datasets keep the same proportion for the explained variable around 11%.


## Fiting the Boosting model 

With the final cleaned dataset, we got from above steps fit our Boosting Model for **y_loan_defaulter** on all **x_variables**.

We made a lot of tests playing with control parameters trying to reduce the errors and we decided to keep **mfinal = 100**, **minbucket = 25** and **maxdepth = 1 **.

```{r fit_boots_model, eval=FALSE}

names  <- names(data.train_boost) # saving the name of all vars to put on formula
f_full <- as.formula(paste("y_loan_defaulter ~",
                           paste(names[!names %in% "y_loan_defaulter"], collapse = " + ")))

boost <- boosting(f_full, data= data.train_boost, mfinal= 100, 
                   coeflearn = "Freund", 
                   control = rpart.control(minbucket= 25,maxdepth = 1))

saveRDS(boost, "./models/boosting.rds")

```

```{r load_boots_model, include=FALSE}

boost <- readRDS("./models/boosting.rds")

```

```{r plot_boots_model, echo=TRUE, out.width= '100%'}

plot(errorevol(boost, data.train_boost))

```

*******************************************************************************

# Interpreting model output

Boosting is a black box ensemble method! But the model can tell us the importance of each variable to predict the results.
For this model the four principal vars are:

- **x_prop_interest_credited**
- **x_account_balance**
- **x_avg_account_balance**
- **x_loan_amount**

The result is similar to the Logistic Regression and Decision Tree models.

*******************************************************************************

# Evaluating the model performance

Here we will perform basically the same steps we did in the Logistic Regression and Decision Tree models.

A comparison against all the models will be provided in the Final Report session of this exercise.

We started this step by making predictions using our model on the X's variables in our Train and Test datasets.

```{r get_scores, echo=TRUE, out.width= '100%'}
## making preditions for each model and consolidating in a single data frame

prob.full = list()
prob.train = list()
prob.test = list()

prob.full$boosting.actual         <- loan_dataset_boost$y_loan_defaulter
prob.full$boosting.predicted      <- predict.boosting(boost, 
                                                      newdata = loan_dataset_boost)$prob[, 2]

prob.train$boosting.actual         <- data.train_boost$y_loan_defaulter
prob.train$boosting.predicted      <- predict.boosting(boost, newdata = 
                                                          data.train_boost)$prob[, 2]

prob.test$boosting.actual         <- data.test_boost$y_loan_defaulter
prob.test$boosting.predicted      <- predict.boosting(boost, newdata = 
                                                         data.test_boost)$prob[, 2]

prob.full   <- prob.full %>% as_tibble()
prob.train  <- prob.train %>% as_tibble()
prob.test   <- prob.test %>% as_tibble()
```

## Getting Performance Measures

To calculate the performance measures, derived from the confusion matrix, we need to find the score cut off that best split our test dataset into Defaulters and Non-Defaulters.

In this exercise we decide to not prioritize the accuracy on predicting Defaulters and Non-Defaulters, therefore we are looking for the score cut off that best predict each class equally.

With the returned object from this function we can plot the comparison between TPR (True Positive Rate) and TNR (True Negative Rate) to find the best cut off.

```{r get_measures, echo=TRUE, out.width= '100%'}
## getting measures -----------------------------------------------------------------
metricsByCutoff.test_boost  <- modelMetrics(prob.test$boosting.actual, 
                                            prob.test$boosting.predicted, 
                                            plot_title = 'Boosting')
metricsByCutoff.test_boost$Plot
```

With the optimized cut off we calculate the full set of model metrics using the function HMeasure from hmeasure library.

```{r get_measures_full, echo=TRUE, out.width= '100%', warning=FALSE}
# Boosting
measures.boosting.train <- HMeasure(prob.train$boosting.actual, 
                                    prob.train$boosting.predicted, 
                                    threshold = metricsByCutoff.test_boost$BestCut['Cut'])
measures.boosting.test  <- HMeasure(prob.test$boosting.actual, 
                                    prob.test$boosting.predicted, 
                                    threshold = metricsByCutoff.test_boost$BestCut['Cut'])


# join measures in a single data frame
measures <- t(bind_rows(measures.boosting.train$metrics,
                        measures.boosting.test$metrics
                        )) %>% as_tibble(., rownames = NA)

colnames(measures) <- c('boosting - train', 'boosting - test')

measures$metric = rownames(measures)

measures <- dplyr::select(measures, metric, everything())
```

Below are the metrics on the train and test dataset:
```{r see_train_measures, echo=TRUE, out.width= '100%'}
kable(measures, row.names = FALSE)
```

## Evaluating classification performance

This model delivered the best discrimination f all models done here (KS = 0.63)! We will compare how it performed against other classes of models in the Final Report session.

Below the confusion matrix and general performance of the model:

```{r see_accuracy_1, echo=TRUE, out.width= '100%'}
# accuracy metrics ---------------------------------------------------------------
# boosting
accuracy(score = prob.test$boosting.predicted, 
         actual = prob.test$boosting.actual, 
         threshold = metricsByCutoff.test_boost[["BestCut"]][["Cut"]])
```

We finally look at the score distribution charts to check how well the model is able to discriminate Defaulters and Non-Defaulters.

```{r model_plots, echo=TRUE, out.width= '100%'}
p1 <- Score_Histograms(prob.test, 
              prob.test$boosting.predicted,
              prob.test$boosting.actual,
             'Density Plot') + theme(axis.title.y = element_blank())

p2 <- Score_Boxplot(prob.test, 
              prob.test$boosting.predicted, 
              prob.test$boosting.actual,
              'Score Boxplot')

p3 <- KS_Plot(prob.test$boosting.predicted[prob.test$boosting.actual == 0],
          prob.test$boosting.predicted[prob.test$boosting.actual == 1],
        'KS Plot') + theme(axis.title.y = element_blank(),
                           axis.text.y = element_blank())

ggarrange(p1, p2, nrow = 2)

p3
```

By the score density we can see that the boosting model is not as narrow as the decision tree on the scores it assigns to the observations.

The box plot can show us a clear discrimination between Defaulters and Non-Defaulters.

The KS metric .63 is considered good for this classification model.

In the Final Report session, we will look more closely on the AUC and Gini metrics by plotting the ROC curve and comparing against other models.

Stay with us!!!
